"""
Weaver Feature - Migrated from module_weaver.py
Tabs: RAG | Debate (Solo + Multi) | History with Bayes
"""

import streamlit as st
import pandas as pd
import numpy as np
import time
import json
from typing import Optional
from datetime import datetime
from engines.ai_engine import AIEngine
from engines.embedding_engine import EmbeddingEngine
from core.i18n_block import I18nBlock
from core.config_block import ConfigBlock
from utils.file_processor import doc_file
from prompts import BOOK_ANALYSIS_PROMPT, DEBATE_PERSONAS
from supabase import create_client, Client
import plotly.express as px


class WeaverFeature:
    def __init__(
        self,
        ai_engine: AIEngine,
        embedding_engine: EmbeddingEngine,
        i18n: Optional[I18nBlock] = None,
        config: Optional[ConfigBlock] = None,
        **kwargs
    ):
        self.ai = ai_engine
        self.embedding = embedding_engine
        self.i18n = i18n
        self.config = config
        
        # Supabase
        self.db = None
        try:
            url = st.secrets.get("supabase", {}).get("url")
            key = st.secrets.get("supabase", {}).get("key")
            if url and key:
                self.db = create_client(url, key)
        except:
            pass
        
        # Session state
        if "weaver_chat" not in st.session_state:
            st.session_state.weaver_chat = []
    
    def t(self, key: str) -> str:
        """Get translation"""
        if self.i18n:
            return self.i18n.t(key, key)
        # Fallback translations
        trans = {
            "tab1": "üìö Ph√¢n T√≠ch S√°ch",
            "tab2": "üó£Ô∏è Tranh Bi·ªán", 
            "tab3": "‚è≥ Nh·∫≠t K√Ω",
            "t1_up_doc": "T·∫£i t√†i li·ªáu (PDF/Docx)",
            "t1_btn": "üöÄ PH√ÇN T√çCH NGAY",
            "t3_persona_label": "Ch·ªçn ƒê·ªëi Th·ªß:",
            "t3_input": "Nh·∫≠p ch·ªß ƒë·ªÅ tranh lu·∫≠n...",
            "t3_clear": "üóëÔ∏è X√≥a Chat"
        }
        return trans.get(key, key)
    
    def render(self):
        st.header("üß† The Cognitive Weaver")
        
        # 3 TABS (b·ªè D·ªãch & Voice)
        tab1, tab2, tab3 = st.tabs([
            self.t("tab1"),  # Ph√¢n T√≠ch S√°ch
            self.t("tab2"),  # Tranh Bi·ªán
            self.t("tab3")   # Nh·∫≠t K√Ω
        ])
        
        with tab1:
            self._render_rag()
        
        with tab2:
            self._render_debate()
        
        with tab3:
            self._render_history()
    
    def _render_rag(self):
        """TAB 1: RAG - Ph√¢n t√≠ch s√°ch"""
        st.subheader("Tr·ª£ l√Ω Nghi√™n c·ª©u")
        
        uploaded_files = st.file_uploader(
            self.t("t1_up_doc"),
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True,
            key="weaver_rag_files"
        )
        
        if st.button(self.t("t1_btn"), type="primary", use_container_width=True):
            if not uploaded_files:
                st.warning("Vui l√≤ng t·∫£i l√™n t√†i li·ªáu")
                return
            
            for f in uploaded_files:
                text = doc_file(f)
                if not text:
                    st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {f.name}")
                    continue
                
                with st.spinner(f"ƒêang ph√¢n t√≠ch {f.name}..."):
                    prompt = f"Ph√¢n t√≠ch t√†i li·ªáu '{f.name}'. N·ªôi dung: {text[:30000]}"
                    
                    # G·ªçi AI v·ªõi priority Gemini Pro
                    response = self.ai.generate(
                        prompt,
                        system_instruction=BOOK_ANALYSIS_PROMPT,
                        model_type="pro"
                    )
                    
                    if response.success:
                        st.markdown(f"### üìÑ {f.name}")
                        st.markdown(response.content)
                        st.markdown("---")
                        
                        # Log
                        self._log_to_supabase(
                            "Ph√¢n T√≠ch S√°ch",
                            f.name,
                            response.content[:500],
                            response.provider
                        )
                    else:
                        st.error(f"‚ùå {response.error}")
    
    def _render_debate(self):
        """TAB 2: Debate - Solo + Multi-Agent"""
        st.subheader("ƒê·∫•u Tr∆∞·ªùng T∆∞ Duy")
        
        mode = st.radio(
            "Mode:",
            ["üë§ Solo", "‚öîÔ∏è Multi-Agent"],
            horizontal=True,
            key="weaver_debate_mode"
        )
        
        # Init chat history
        if "weaver_chat" not in st.session_state:
            st.session_state.weaver_chat = []
        
        # ========== MODE 1: SOLO ==========
        if mode == "üë§ Solo":
            c1, c2 = st.columns([3, 1])
            
            with c1:
                persona = st.selectbox(
                    self.t("t3_persona_label"),
                    list(DEBATE_PERSONAS.keys()),
                    key="weaver_solo_persona"
                )
            
            with c2:
                if st.button(self.t("t3_clear"), key="weaver_solo_clear"):
                    st.session_state.weaver_chat = []
                    st.rerun()
            
            # Hi·ªÉn th·ªã history
            for msg in st.session_state.weaver_chat:
                st.chat_message(msg["role"]).write(msg["content"])
            
            # Input
            if prompt := st.chat_input(self.t("t3_input")):
                # Add user message
                st.chat_message("user").write(prompt)
                st.session_state.weaver_chat.append({
                    "role": "user",
                    "content": prompt
                })
                
                # Build context from history
                recent = st.session_state.weaver_chat[-10:]
                context = "\n".join([
                    f"{m['role'].upper()}: {m['content']}"
                    for m in recent
                ])
                
                full_prompt = f"""
L·ªäCH S·ª¨ H·ªòI THO·∫†I:
{context}

NHI·ªÜM V·ª§: D·ª±a v√†o l·ªãch s·ª≠ tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa USER.
N·∫øu USER h·ªèi "c√¢u h·ªèi c≈©" ho·∫∑c "v·ª´a r·ªìi", h√£y tham chi·∫øu ƒë·∫øn l·ªãch s·ª≠.
"""
                
                with st.chat_message("assistant"):
                    with st.spinner("ü§î ƒêang suy nghƒ©..."):
                        # ∆Øu ti√™n Gemini Pro
                        response = self.ai.generate(
                            full_prompt,
                            system_instruction=DEBATE_PERSONAS[persona],
                            model_type="pro"
                        )
                        
                        if response.success:
                            st.write(response.content)
                            
                            # Save assistant response
                            st.session_state.weaver_chat.append({
                                "role": "assistant",
                                "content": response.content
                            })
                            
                            # Log
                            full_log = f"üë§ USER: {prompt}\n\nü§ñ {persona}: {response.content}"
                            self._log_to_supabase(
                                "Tranh Bi·ªán Solo",
                                f"{persona} - {prompt[:50]}...",
                                full_log,
                                response.provider
                            )
                        else:
                            st.error(f"‚ùå {response.error}")
        
        # ========== MODE 2: MULTI-AGENT ==========
        else:
            st.info("üí° Ch·ªçn 2-3 nh√¢n v·∫≠t ƒë·ªÉ h·ªç t·ª± tranh lu·∫≠n.")
            
            participants = st.multiselect(
                "Ch·ªçn H·ªôi ƒë·ªìng Tranh Bi·ªán:",
                list(DEBATE_PERSONAS.keys()),
                default=list(DEBATE_PERSONAS.keys())[:2],
                max_selections=3,
                key="weaver_multi_participants"
            )
            
            topic = st.text_input(
                "Ch·ªß ƒë·ªÅ tranh lu·∫≠n:",
                placeholder="VD: Ti·ªÅn c√≥ mua ƒë∆∞·ª£c h·∫°nh ph√∫c kh√¥ng?",
                key="weaver_multi_topic"
            )
            
            c_start, c_del = st.columns([1, 5])
            
            with c_start:
                start_btn = st.button(
                    "üî• KHAI CHI·∫æN",
                    key="weaver_multi_start",
                    disabled=(len(participants) < 2 or not topic),
                    type="primary"
                )
            
            with c_del:
                if st.button("üóëÔ∏è X√≥a B√†n", key="weaver_multi_clear"):
                    st.session_state.weaver_chat = []
                    st.rerun()
            
            # Hi·ªÉn th·ªã history c≈©
            for msg in st.session_state.weaver_chat:
                if msg["role"] == "system":
                    st.info(msg["content"])
                else:
                    st.chat_message("assistant").write(msg["content"])
            
            # Ch·∫°y debate
            if start_btn and topic and len(participants) >= 2:
                # Reset
                st.session_state.weaver_chat = []
                
                # M·ªü ƒë·∫ßu
                start_msg = f"üì¢ **CH·ª¶ T·ªåA:** Khai m·∫°c tranh lu·∫≠n v·ªÅ: *'{topic}'*"
                st.session_state.weaver_chat.append({
                    "role": "system",
                    "content": start_msg
                })
                st.info(start_msg)
                
                # Full transcript
                transcript = [start_msg]
                
                with st.status("üî• Cu·ªôc chi·∫øn ƒëang di·ªÖn ra (3 v√≤ng)...") as status:
                    for round_num in range(1, 4):
                        status.update(label=f"üîÑ V√≤ng {round_num}/3...")
                        
                        for p_name in participants:
                            # Context
                            if len(st.session_state.weaver_chat) > 1:
                                recent = st.session_state.weaver_chat[-3:]
                                context_str = "\n".join([
                                    f"- {m['content']}"
                                    for m in recent
                                    if m['role'] != 'system'
                                ])
                            else:
                                context_str = topic
                            
                            # Build prompt
                            if round_num == 1:
                                p_prompt = f"""
CH·ª¶ ƒê·ªÄ TRANH LU·∫¨N: {topic}

NHI·ªÜM V·ª§ (V√≤ng 1 - Khai m·∫°c):
B·∫°n l√† {p_name}. H√£y ƒë∆∞a ra quan ƒëi·ªÉm m·ªü ƒë·∫ßu c·ªßa m√¨nh v·ªÅ ch·ªß ƒë·ªÅ n√†y.
N√™u r√µ l·∫≠p tr∆∞·ªùng v√† 2-3 l√Ω l·∫Ω ch√≠nh (d∆∞·ªõi 200 t·ª´).
"""
                            else:
                                p_prompt = f"""
CH·ª¶ ƒê·ªÄ: {topic}

T√åNH HU·ªêNG HI·ªÜN T·∫†I:
{context_str}

NHI·ªÜM V·ª§ (V√≤ng {round_num} - Ph·∫£n bi·ªán):
B·∫°n l√† {p_name}. H√£y:
1. Ch·ªâ ra ƒëi·ªÉm y·∫øu trong l·∫≠p lu·∫≠n c·ªßa ƒë·ªëi th·ªß
2. C·ªßng c·ªë quan ƒëi·ªÉm c·ªßa m√¨nh
3. ƒê∆∞a ra th√™m 1 v√≠ d·ª• minh h·ªça
(D∆∞·ªõi 200 t·ª´, s√∫c t√≠ch)
"""
                            
                            # Call AI (∆Øu ti√™n Gemini Pro)
                            try:
                                response = self.ai.generate(
                                    p_prompt,
                                    system_instruction=DEBATE_PERSONAS[p_name],
                                    model_type="pro"
                                )
                                
                                if response.success:
                                    content_fmt = f"**{p_name}:** {response.content}"
                                    
                                    # Save
                                    st.session_state.weaver_chat.append({
                                        "role": "assistant",
                                        "content": content_fmt
                                    })
                                    
                                    transcript.append(content_fmt)
                                    
                                    # Display
                                    with st.chat_message("assistant"):
                                        st.write(content_fmt)
                                    
                                    # Ngh·ªâ tr√°nh rate limit
                                    time.sleep(6)
                            
                            except Exception as e:
                                st.error(f"‚ö†Ô∏è L·ªói {p_name}: {str(e)}")
                    
                    status.update(label="‚úÖ Tranh lu·∫≠n k·∫øt th√∫c!", state="complete")
                
                # Log
                full_log = "\n\n".join(transcript)
                self._log_to_supabase(
                    "H·ªôi ƒë·ªìng Tranh Bi·ªán",
                    f"Ch·ªß ƒë·ªÅ: {topic}",
                    full_log
                )
                
                st.toast("üíæ ƒê√£ l∆∞u bi√™n b·∫£n v√†o Nh·∫≠t K√Ω!", icon="‚úÖ")
                
                # Xem to√†n b·ªô
                with st.expander("üìÑ Xem To√†n B·ªô Bi√™n B·∫£n", expanded=False):
                    st.markdown(full_log)
    
    def _render_history(self):
        """TAB 3: Nh·∫≠t K√Ω v·ªõi Bayesian Analysis"""
        st.subheader("‚è≥ Nh·∫≠t K√Ω & Ph·∫£n Chi·∫øu T∆∞ Duy")
        
        col_btn1, col_btn2 = st.columns([1, 4])
        with col_btn1:
            if st.button("üîÑ T·∫£i l·∫°i", key="weaver_history_refresh"):
                if 'history_cloud' in st.session_state:
                    del st.session_state['history_cloud']
                st.rerun()
        
        # Load data
        data = self._load_history()
        
        if data:
            df_h = pd.DataFrame(data)
            
            # Bi·ªÉu ƒë·ªì c·∫£m x√∫c (n·∫øu c√≥)
            if "sentiment_score" in df_h.columns:
                try:
                    df_h["score"] = pd.to_numeric(df_h["sentiment_score"], errors='coerce').fillna(0)
                    
                    st.caption("üìâ Bi·ªÉu ƒë·ªì dao ƒë·ªông tr·∫°ng th√°i c·∫£m x√∫c/t∆∞ duy:")
                    fig = px.line(
                        df_h,
                        x="created_at",
                        y="score",
                        markers=True,
                        color_discrete_sequence=["#76FF03"],
                        labels={"score": "Ch·ªâ s·ªë T√≠ch c·ª±c", "created_at": "Th·ªùi gian"}
                    )
                    fig.update_layout(height=250, margin=dict(l=20, r=20, t=10, b=20))
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass
            
            # Bayesian Analysis
            with st.expander("üîÆ Ph√¢n t√≠ch T∆∞ duy theo Bayes (E.T. Jaynes)", expanded=False):
                st.info("AI s·∫Ω coi L·ªãch s·ª≠ ho·∫°t ƒë·ªông l√† 'Evidence' ƒë·ªÉ suy lu·∫≠n ra 'Objective Function' v√† s·ª± d·ªãch chuy·ªÉn ni·ªÅm tin.")
                
                if st.button("üß† Ch·∫°y M√¥ h√¨nh Bayes", key="weaver_bayes_run"):
                    with st.spinner("ƒêang t√≠nh to√°n x√°c su·∫•t h·∫≠u nghi·ªám (Posterior)..."):
                        # L·∫•y 10 logs g·∫ßn nh·∫•t
                        recent_logs = df_h.tail(10).to_dict(orient="records")
                        logs_text = json.dumps(recent_logs, ensure_ascii=False, indent=2)
                        
                        bayes_prompt = f"""
ƒê√≥ng vai m·ªôt nh√† khoa h·ªçc t∆∞ duy theo tr∆∞·ªùng ph√°i E.T. Jaynes (s√°ch 'Probability Theory: The Logic of Science').

D·ªÆ LI·ªÜU QUAN S√ÅT (EVIDENCE):
ƒê√¢y l√† nh·∫≠t k√Ω ho·∫°t ƒë·ªông:
{logs_text}

NHI·ªÜM V·ª§:
H√£y ph√¢n t√≠ch chu·ªói h√†nh ƒë·ªông n√†y nh∆∞ m·ªôt b√†i to√°n suy lu·∫≠n Bayes.
1. **X√°c ƒë·ªãnh Priors (Ni·ªÅm tin ti√™n nghi·ªám):** D·ª±a tr√™n c√°c h√†nh ƒë·ªông ƒë·∫ßu, t√¥i ƒëang quan t√¢m/tin t∆∞·ªüng ƒëi·ªÅu g√¨?
2. **C·∫≠p nh·∫≠t Likelihood (Kh·∫£ nƒÉng):** C√°c h√†nh ƒë·ªông ti·∫øp theo c·ªßng c·ªë hay l√†m y·∫øu ƒëi ni·ªÅm tin ƒë√≥?
3. **K·∫øt lu·∫≠n Posterior (H·∫≠u nghi·ªám):** Tr·∫°ng th√°i t∆∞ duy hi·ªán t·∫°i ƒëang h·ªôi t·ª• v·ªÅ ƒë√¢u? C√≥ m√¢u thu·∫´n (Inconsistency) n√†o trong logic h√†nh ƒë·ªông kh√¥ng?

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√¢u s·∫Øc, d√πng thu·∫≠t ng·ªØ x√°c su·∫•t nh∆∞ng d·ªÖ hi·ªÉu.
"""
                        
                        # Call AI Pro
                        response = self.ai.generate(
                            bayes_prompt,
                            model_type="pro"
                        )
                        
                        if response.success:
                            st.markdown(response.content)
                        else:
                            st.error(f"‚ùå {response.error}")
            
            # Danh s√°ch chi ti·∫øt
            st.divider()
            st.write("üìú **Chi ti·∫øt Nh·∫≠t k√Ω:**")
            
            # ƒê·∫£o ng∆∞·ª£c ƒë·ªÉ xem m·ªõi nh·∫•t tr∆∞·ªõc
            for _, item in df_h.iloc[::-1].iterrows():
                time_str = str(item.get('created_at', ''))[:19]
                type_str = str(item.get('type', ''))
                title_str = str(item.get('title', ''))
                content_str = str(item.get('content', ''))
                provider = str(item.get('provider', ''))
                
                # Icon
                icon = "üìï"
                if "Tranh Bi·ªán" in type_str:
                    icon = "üó£Ô∏è"
                elif "D·ªãch" in type_str:
                    icon = "‚úçÔ∏è"
                elif "Audio" in type_str:
                    icon = "üéôÔ∏è"
                
                # Provider badge
                provider_badge = ""
                if provider:
                    icon_map = {"gemini": "üü°", "grok": "üü¢", "deepseek": "üü£"}
                    provider_badge = f" {icon_map.get(provider, '‚ö´')} {provider.upper()}"
                
                # Truncate content cho preview
                preview = content_str[:100] + "..." if len(content_str) > 100 else content_str
                
                # Expander
                with st.expander(
                    f"{icon} {time_str} | {type_str} | {title_str}{provider_badge}",
                    expanded=False
                ):
                    st.markdown(content_str)
                    
                    # Sentiment n·∫øu c√≥
                    if 'sentiment_label' in item and item['sentiment_label']:
                        st.caption(f"C·∫£m x√∫c: {item['sentiment_label']} ({item.get('sentiment_score', 0)})")
        else:
            st.info("üì≠ Ch∆∞a c√≥ d·ªØ li·ªáu l·ªãch s·ª≠.")
    
    def _load_history(self):
        """Load history t·ª´ Supabase"""
        if 'history_cloud' in st.session_state:
            return st.session_state.history_cloud
        
        if not self.db:
            return []
        
        try:
            response = self.db.table("history_logs").select("*").order("created_at", desc=True).limit(50).execute()
            data = response.data or []
            st.session_state.history_cloud = data
            return data
        except:
            return []
    
    def _log_to_supabase(self, type_str, title, content, provider=None):
        """Log v√†o Supabase"""
        if not self.db:
            return
        
        try:
            data = {
                "type": type_str,
                "title": title,
                "content": content,
                "user_name": st.session_state.get("current_user", "Guest")
            }
            
            if provider:
                data["provider"] = provider
            
            self.db.table("history_logs").insert(data).execute()
        except:
            pass
